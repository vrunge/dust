
#' dust_R_2param
#'
#' @description DUST algorithm for univariate time-series (with different possible data models)
#' @param data a vector of data (type \code{"meanVar"}) generated by \code{dataGenerator_MV} or a data frame of data of dimension n x 2 (type \code{"regression"}) generated by \code{dataGenerator_Reg} with column named \code{x} and \code{y}.
#' @param penalty penalty value (non-negative)
#' @param type type of cost to use: \code{"meanVar"}, \code{"regression"}
#' @param pruningOpt the pruning option to use. 0 = nothing, 1 = PELT, 2 = dust, 3 = PELT + dust
#' @return a list with (1) the change-point elements (each last index of each segment in \code{changepoints}), (2) a vector `\code{nb} saving the number of non-pruned elements at each iteration, (3) a vector \code{lastIndexSet} containing the non-pruned indices at the end of the algo and (4) a vector \code{costQ} saving the optimal cost at each time step.
#' @examples
#' data <- dataGenerator_MV(c(50,100), c(0,1), c(0.5,0.2))
#' dust_R_2param(data, 4*log(100), type = "meanVar", pruningOpt = 1)
#' dust_R_2param(data, 4*log(100), type = "meanVar", pruningOpt = 2)
#'
#' data <- dataGenerator_Reg(chpts = c(50,100), A = c(1,-1), B = c(2,0))
#' dust_R_2param(data, 4*log(100), type = "regression", pruningOpt = 1)
#' dust_R_2param(data, 4*log(100), type = "regression", pruningOpt = 2)
#'
dust_R_2param <- function(data,
                          penalty,
                          type = "meanVar",
                          pruningOpt = 2)
{
  ## pruningOpt == 0: nothing
  ## pruningOpt == 1: PELT
  ## pruningOpt == 2: dust
  ## pruningOpt == 3: dust + PELT

  allowed.types <- c("meanVar", "regression")
  if(!type %in% allowed.types){stop('type must be one of: ', paste(allowed.types, collapse=", "))}
  if(penalty < 0){stop('penalty must be non negative')}

  if(type == "meanVar")
  {
    if(!is.vector(data)){stop('data is not a vector')}
    if(length(data) <= 1){stop('no data to segment')}
    res <- dust_R_2param_meanVar(data, penalty, pruningOpt)
  }

  if(type == "regression")
  {
    if(!is.data.frame(data)){stop('data is not a data frame')}
    if(nrow(data) <= 1){stop('no data to segment')}
    if(ncol(data) != 2){stop('You must have only 2 columns: x and y')}
    if(all(colnames(data) != c("x","y"))){'colum names are not x and y'}
    res <- dust_R_2param_regression(data, penalty, pruningOpt)
  }
  return(res)
}


##########################################################################################
##########################################################################################
##########################################################################################


dust_R_2param_meanVar <- function(data, penalty, pruningOpt)
{
  ###
  ### preprocessing
  ###
  ### S and S2 used with the shift operator
  ###
  n <- length(data)
  S <- c(0, cumsum(data))
  S2 <- c(0, cumsum(data^2))
  #########
  ###
  ### INITIALIZATION
  ###
  ### costQ used with the shift operator
  ###
  cp <- rep(0, n) #cp vector cp[k] = index of the last change-point for data y(1) to y(k)
  costQ <- rep(0, n + 1) # costQ[k] optimal cost for data y(1) to y(k-1)
  nb <- rep(0, n) # number of element to consider at each iteration
  costQ[1] <- -penalty
  indexSet <- 0
  #########
  ###
  ### update rule Dynamic Programming
  ###
  for(t in 1:n) # at t, transform Q_{t-1} into Q_{t}
  {
    min_temp <- Inf
    index <- 0 #if all eval = Inf (start of a segment)
    for(i in indexSet)
    {
      ### weight (t-i) --- cost Si..t = S_i+1 to S_t --- costQ[i] = Q_(i) (Q_0 = -penalty) ###
      eval <- min_cost_meanVar(S, S2, shift(i), shift(t), costQ[shift(i)] + penalty)
      if(eval < min_temp){min_temp <- eval; index <- i}
    }
    costQ[shift(t)] <- min_temp
    cp[t] <- index

    ###########################
    ########################### Pruning step START
    ###########################

    if(length(indexSet) > 1)
    {
      PrunedIndexSet <- NULL ##### we keep the smallest index #####
      for(i in 1:length(indexSet))
      {
        s1 <- indexSet[i]

        if((pruningOpt == 2) || (pruningOpt == 3))  ### dust
        {
          if(i >= 2)  ### DUST
          {
            if(t - s1 > 1) # if one value in the segment, don't prune !!!!!
            {
              if(i == 2){s2 <- indexSet[1]}else{s2 <- sample(indexSet[1:(i-1)], 1)} # the index of the constraint (s2 < s1) and s1,s2 < t
              if(s1 - s2 > 1) ### REMOVE LATER
              {
                mu <- runif(1) * mu_max_2param(S, S2, shift(s1), shift(s2), shift(t))# the mu value to test
                val <- evalDual_meanVar(mu, S, S2, shift(s1), shift(s2), shift(t), costQ[shift(s1)] + penalty, costQ[shift(s2)] + penalty)
                #print(mu)
                #print(val)
                if(val > costQ[shift(t)] + penalty){PrunedIndexSet <- c(PrunedIndexSet, s1)}
              }

            }
          }
        }

        if((pruningOpt == 1) || (pruningOpt == 3))  ### PELT additional pruning
        {
          if(t - s1 > 1) # if one value in the segment, don't prune !!!!!
          {
            val <- min_cost_meanVar(S, S2, shift(s1), shift(t), costQ[shift(s1)] + penalty)
            if(val > costQ[shift(t)] + penalty){PrunedIndexSet <- c(PrunedIndexSet, s1)}
          }
        }

      }
      indexSet <- setdiff(indexSet, PrunedIndexSet)
    }
    ###########################
    ########################### Pruning step END
    ###########################

    nb[t] <- length(indexSet)
    indexSet <- c(indexSet, t) #add new test point
  }

  ##### ##### ##### ##### ##### ##### ##### ##### ##### ##### ##### ##### #####
  #########
  ###
  ### backtracking step
  ###
  changepoints <- n # vector of change-point to build
  current <- n

  while(changepoints[1] > 0)
  {
    pointval <- cp[current] #new last change
    changepoints <- c(pointval, changepoints) # update vector
    current <- pointval
  }
  return(list(changepoints = changepoints[-1], nb = nb, lastIndexSet = indexSet, costQ = costQ[-1]))
}



##########################################################################################
##########################################################################################
##########################################################################################


dust_R_2param_regression <- function(data, penalty, pruningOpt)
{
  ###
  ### preprocessing
  ###
  ### A,B,C,D,E,f used with the shift operator
  ###
  n <- nrow(data)
  A <- c(0, cumsum(data$x^2))
  B <- c(0, cumsum(data$x))
  C <- 0:n
  D <- -c(0, cumsum(data$x * data$y))
  E <- -c(0, cumsum(data$y))
  f <- c(0, cumsum(data$y^2))

  #########
  ###
  ### INITIALIZATION
  ###
  ### costQ used with the shift operator
  ###
  cp <- rep(0, n) #cp vector cp[k] = index of the last change-point for data y(1) to y(k)
  costQ <- rep(0, n + 1) # costQ[k] optimal cost for data y(1) to y(k-1)
  nb <- rep(0, n) # number of element to consider at each iteration
  costQ[1] <- -penalty
  indexSet <- 0
  #########
  ###
  ### update rule Dynamic Programming
  ###
  for(t in 1:n) # at t, transform Q_{t-1} into Q_{t}
  {
    min_temp <- Inf
    index <- 0 #if all eval = Inf (start of a segment)
    for(i in indexSet)
    {
      ### weight (t-i) --- cost Si..t = S_i+1 to S_t --- costQ[i] = Q_(i) (Q_0 = -penalty) ###
      eval <- min_cost_regression(A,B,C,D,E,f, shift(i), shift(t), costQ[shift(i)] + penalty)
      if(eval < min_temp){min_temp <- eval; index <- i}
    }
    costQ[shift(t)] <- min_temp
    cp[t] <- index

    ###########################
    ########################### Pruning step START
    ###########################

    if(length(indexSet) > 1)
    {
      PrunedIndexSet <- NULL ##### we keep the smallest index #####
      for(i in 1:length(indexSet))
      {
        s1 <- indexSet[i]

        if((pruningOpt == 2) || (pruningOpt == 3))  ### dust
        {
          if(i >= 2)  ### DUST
          {
            if(t - s1 > 1) # if one value in the segment, don't prune !!!!!
            {
              if(i == 2){s2 <- indexSet[1]}else{s2 <- sample(indexSet[1:(i-1)], 1)} # the index of the constraint (s2 < s1) and s1,s2 < t
              if(s1 - s2 > 1) ### REMOVE LATER
              {
                mu <- runif(1) * mu_max_2param(B, A, shift(s1), shift(s2), shift(t))# the mu value to test
                val <- evalDual_regression(mu, A,B,C,D,E,f, shift(s1), shift(s2), shift(t), costQ[shift(s1)] + penalty, costQ[shift(s2)] + penalty)
                #print(mu)
                #print(val)
                if(val > costQ[shift(t)] + penalty){PrunedIndexSet <- c(PrunedIndexSet, s1)}
              }

            }
          }
        }

        if((pruningOpt == 1) || (pruningOpt == 3))  ### PELT additional pruning
        {
          if(t - s1 > 1) # if one value in the segment, don't prune !!!!!
          {
            val <- min_cost_regression(A,B,C,D,E,f, shift(s1), shift(t), costQ[shift(s1)] + penalty)
            if(val > costQ[shift(t)] + penalty){PrunedIndexSet <- c(PrunedIndexSet, s1)}
          }
        }

      }
      indexSet <- setdiff(indexSet, PrunedIndexSet)
    }
    ###########################
    ########################### Pruning step END
    ###########################

    nb[t] <- length(indexSet)
    indexSet <- c(indexSet, t) #add new test point
  }

  ##### ##### ##### ##### ##### ##### ##### ##### ##### ##### ##### ##### #####
  #########
  ###
  ### backtracking step
  ###
  changepoints <- n # vector of change-point to build
  current <- n

  while(changepoints[1] > 0)
  {
    pointval <- cp[current] #new last change
    changepoints <- c(pointval, changepoints) # update vector
    current <- pointval
  }
  return(list(changepoints = changepoints[-1], nb = nb, lastIndexSet = indexSet, costQ = costQ[-1]))
}



