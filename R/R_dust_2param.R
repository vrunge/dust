
#' dust_R_2param
#'
#' @description DUST algorithm for univariate time-series (with different possible data models)
#' @param data a vector of data (type \code{"meanVar"}) generated by \code{dataGenerator_meanVar} or a data frame of data of dimension n x 2 (type \code{"regression"}) generated by \code{dataGenerator_Reg} with column named \code{x} and \code{y}.
#' @param penalty penalty value (non-negative)
#' @param type type of cost to use: \code{"meanVar"}, \code{"regression"}
#' @param pruningOpt the pruning option to use. 0 = nothing, 1 = PELT, 2 = dust, 3 = PELT + dust
#' @return a list with (1) the change-point elements (each last index of each segment in \code{changepoints}), (2) a vector `\code{nb} saving the number of non-pruned elements at each iteration, (3) a vector \code{lastIndexSet} containing the non-pruned indices at the end of the algo and (4) a vector \code{costQ} saving the optimal cost at each time step.
#' @examples
#' data <- dataGenerator_meanVar(c(50,100), c(0,1), c(0.5,0.2))
#' dust_R_2param(data, 4*log(100), type = "meanVar", pruningOpt = 1)
#' dust_R_2param(data, 4*log(100), type = "meanVar", pruningOpt = 2)
#'
#' data <- dataGenerator_Reg(chpts = c(50,100), A = c(1,-1), B = c(2,0))
#' dust_R_2param(data, 4*log(100), type = "regression", pruningOpt = 1)
#' dust_R_2param(data, 4*log(100), type = "regression", pruningOpt = 2)
dust_R_2param <- function(data,
                          penalty,
                          type = "meanVarExact",
                          pruningOpt = 2)
{
  ## pruningOpt == 0: nothing
  ## pruningOpt == 1: PELT
  ## pruningOpt == 2: dust
  ## pruningOpt == 3: dust + PELT

  allowed.types <- c("meanVar", "meanVarExact", "meanVarExact2", "regression")
  if(!type %in% allowed.types){stop('type must be one of: ', paste(allowed.types, collapse=", "))}
  if(penalty < 0){stop('penalty must be non negative')}

  if(type == "meanVar")
  {
    if(!is.vector(data)){stop('data is not a vector')}
    if(length(data) <= 1){stop('no data to segment')}
    res <- dust_R_2param_meanVar(data, penalty, pruningOpt)
  }
  if(type == "meanVarExact")
  {
    if(!is.vector(data)){stop('data is not a vector')}
    if(length(data) <= 1){stop('no data to segment')}
    res <- dust_R_2param_meanVarExact(data, penalty)
  }
  if(type == "meanVarExact2")
  {
    if(!is.vector(data)){stop('data is not a vector')}
    if(length(data) <= 1){stop('no data to segment')}
    res <- dust_R_2param_meanVarExact2(data, penalty)
  }

  if(type == "regression")
  {
    if(!is.data.frame(data)){stop('data is not a data frame')}
    if(nrow(data) <= 1){stop('no data to segment')}
    if(ncol(data) != 2){stop('You must have only 2 columns: x and y')}
    if(all(colnames(data) != c("x","y"))){'colum names are not x and y'}
    res <- dust_R_2param_regression(data, penalty, pruningOpt)
  }
  return(res)
}


##########################################################################################
##########################################################################################
##########################################################################################


#' dust_R_2param_meanVar
#'
#' @description DUST algorithm for meanVar problem
#' @param data a vector of data (type \code{"meanVar"}) generated by \code{dataGenerator_meanVar}
#' @param penalty penalty value (non-negative)
#' @param pruningOpt the pruning option to use. 0 = nothing, 1 = PELT, 2 = dust, 3 = PELT + dust
dust_R_2param_meanVar <- function(data, penalty, pruningOpt)
{
  ###
  ### preprocessing
  ###
  ### S and S2 used with the shift operator
  ###
  n <- length(data)
  S <- c(0, cumsum(data))
  S2 <- c(0, cumsum(data^2))
  #########
  ###
  ### INITIALIZATION
  ###
  ### costQ used with the shift operator
  ###
  cp <- rep(0, n) #cp vector cp[k] = index of the last change-point for data y(1) to y(k)
  costQ <- rep(0, n + 1) # costQ[k] optimal cost for data y(1) to y(k-1)
  nb <- rep(0, n) # number of element to consider at each iteration
  costQ[1] <- -penalty
  indexSet <- 0
  #########
  ###
  ### update rule Dynamic Programming
  ###
  for(t in 1:n) # at t, transform Q_{t-1} into Q_{t}
  {
    min_temp <- Inf
    index <- 0 #if all eval = Inf (start of a segment)
    for(i in indexSet)
    {
      ### weight (t-i) --- cost Si..t = S_i+1 to S_t --- costQ[i] = Q_(i) (Q_0 = -penalty) ###
      eval <- min_cost_meanVar(S, S2, shift(i), shift(t), costQ[shift(i)] + penalty)
      if(eval < min_temp){min_temp <- eval; index <- i}
    }
    costQ[shift(t)] <- min_temp
    cp[t] <- index

    ###########################
    ########################### Pruning step START
    ###########################

    if(length(indexSet) > 1)
    {
      PrunedIndexSet <- NULL ##### we keep the smallest index #####
      for(i in 1:length(indexSet))
      {
        s1 <- indexSet[i]

        if((pruningOpt == 2) || (pruningOpt == 3))  ### dust
        {
          if(i >= 2)  ### DUST
          {
            if(t - s1 > 1) # if one value in the segment, don't prune !!!!!
            {
              if(i == 2){s2 <- indexSet[1]}else{s2 <- sample(indexSet[1:(i-1)], 1)} # the index of the constraint (s2 < s1) and s1,s2 < t
              if(s1 - s2 > 1) ### REMOVE LATER
              {
                mu <- runif(1) * mu_max_2param(S, S2, shift(s1), shift(s2), shift(t))# the mu value to test
                val <- evalDual_meanVar(mu, S, S2, shift(s1), shift(s2), shift(t), costQ[shift(s1)] + penalty, costQ[shift(s2)] + penalty)
                #print(mu)
                #print(val)
                if(val > costQ[shift(t)] + penalty){PrunedIndexSet <- c(PrunedIndexSet, s1)}
              }

            }
          }
        }
        if((pruningOpt == 1) || (pruningOpt == 3))  ### PELT additional pruning
        {
          if(t - s1 > 1) # if one value in the segment, don't prune !!!!!
          {
            val <- min_cost_meanVar(S, S2, shift(s1), shift(t), costQ[shift(s1)] + penalty)
            if(val > costQ[shift(t)] + penalty){PrunedIndexSet <- c(PrunedIndexSet, s1)}
          }
        }

      }
      indexSet <- setdiff(indexSet, PrunedIndexSet)
    }
    ###########################
    ########################### Pruning step END
    ###########################

    nb[t] <- length(indexSet)
    indexSet <- c(indexSet, t) #add new test point
  }

  ########## backtracking changepoint ##########
  changepoints <- backtracking_changepoint(cp, n)
  ########## backtracking changepoint ##########

  return(list(changepoints = changepoints, nb = nb, lastIndexSet = indexSet, costQ = costQ[-1]))
}


##########################################################################################

##########################################################################################

##########################################################################################

#' dust_R_2param_meanVarExact
#'
#' @description DUST algorithm for meanVar problem
#' @param data a vector of data (type \code{"meanVar"}) generated by \code{dataGenerator_meanVar}
#' @param penalty penalty value (non-negative)
dust_R_2param_meanVarExact <- function(data, penalty)
{
  ###
  ### preprocessing
  ###
  ### S and S2 used with the shift operator
  ###
  n <- length(data)
  S <- c(0, cumsum(data))
  S2 <- c(0, cumsum(data^2))
  #########
  ###
  ### INITIALIZATION
  ###
  ### costQ used with the shift operator
  ###
  cp <- rep(0, n) #cp vector cp[k] = index of the last change-point for data y(1) to y(k)
  costQ <- rep(0, n + 1) # costQ[k] optimal cost for data y(1) to y(k-1)
  nb <- rep(0, n) # number of element to consider at each iteration
  costQ[1] <- -penalty
  indexSet <- 0
  #########
  ###
  ### update rule Dynamic Programming
  ###
  for(t in 1:n) # at t, transform Q_{t-1} into Q_{t}
  {
    min_temp <- Inf
    index <- 0 #if all eval = Inf (start of a segment)
    for(i in indexSet)
    {
      ### weight (t-i) --- cost Si..t = S_i+1 to S_t --- costQ[i] = Q_(i) (Q_0 = -penalty) ###
      eval <- min_cost_meanVar(S, S2, shift(i), shift(t), costQ[shift(i)] + penalty)
      if(eval < min_temp){min_temp <- eval; index <- i}
    }
    costQ[shift(t)] <- min_temp
    cp[t] <- index

    ###########################
    ###########################  Pruning step START
    ###########################

    if(length(indexSet) > 1)
    {
      PrunedIndexSet <- NULL
      ###
      ### DUST
      ###
      for(i in 2:length(indexSet))
      {
        s <- indexSet[i]
        if(t - s > 1) # if one value in the segment, don't prune !!!!!
        {
          if(i == 2){r <- indexSet[1]}else{r <- sample(indexSet[1:(i-1)], 1)}
          r <- indexSet[i-1]
          {
            mu <- compute_xstar(S, S2,
                                shift(r), shift(s), shift(t),
                                costQ[shift(r)], costQ[shift(s)], costQ[shift(t)])
            val <- evalDual_meanVar2(mu, S, S2,
                                     shift(r), shift(s), shift(t),
                                     costQ[shift(r)], costQ[shift(s)], costQ[shift(t)])
            if(val > 0){PrunedIndexSet <- c(PrunedIndexSet, s)}
          }
        }
      }

      ###
      ### PELT for smallest index OK
      ###
      s <- indexSet[1]
      if(t - s > 1) # if one value in the segment, don't prune !!!!!
      {
        val <- min_cost_meanVar(S, S2, shift(s), shift(t), costQ[shift(s)] + penalty)
        if(val > costQ[shift(t)] + penalty)
        {PrunedIndexSet <- c(PrunedIndexSet, s)}
      }
      ###
      ### new index set
      ###
      indexSet <- setdiff(indexSet, PrunedIndexSet)
    }
    ###########################
    ########################### Pruning step END
    ###########################

    nb[t] <- length(indexSet)
    indexSet <- c(indexSet, t) #add new test point
  }

  ########## backtracking changepoint ##########
  changepoints <- backtracking_changepoint(cp, n)
  ########## backtracking changepoint ##########

  return(list(changepoints = changepoints, nb = nb, lastIndexSet = indexSet, costQ = costQ[-1]))
}





#' dust_R_2param_meanVarExact2
#'
#' @description DUST algorithm for meanVar problem
#' @param data a vector of data (type \code{"meanVar"}) generated by \code{dataGenerator_meanVar}
#' @param penalty penalty value (non-negative)
dust_R_2param_meanVarExact2 <- function(data, penalty)
{
  ###
  ### preprocessing
  ###
  ### S and S2 used with the shift operator
  ###
  n <- length(data)
  S <- c(0, cumsum(data))
  S2 <- c(0, cumsum(data^2))
  #########
  ###
  ### INITIALIZATION
  ###
  ### costQ used with the shift operator
  ###
  cp <- rep(0, n) #cp vector cp[k] = index of the last change-point for data y(1) to y(k)
  costQ <- rep(0, n + 1) # costQ[k] optimal cost for data y(1) to y(k-1)
  nb <- rep(0, n) # number of element to consider at each iteration
  costQ[1] <- -penalty
  indexSet <- 0

  dualPruning <- c(0, 0, 0, 0) #0,r1,r2,r1+R2
  #########
  ###
  ### update rule Dynamic Programming
  ###
  for(t in 1:n) # at t, transform Q_{t-1} into Q_{t}
  {
    min_temp <- Inf
    index <- 0 #if all eval = Inf (start of a segment)
    for(i in indexSet)
    {
      ### weight (t-i) --- cost Si..t = S_i+1 to S_t --- costQ[i] = Q_(i) (Q_0 = -penalty) ###
      eval <- min_cost_meanVar(S, S2, shift(i), shift(t), costQ[shift(i)] + penalty)
      if(eval < min_temp){min_temp <- eval; index <- i}
    }
    costQ[shift(t)] <- min_temp
    cp[t] <- index

    ###########################
    ###########################  Pruning step START
    ###########################

    if(length(indexSet) > 2)
    {
      PrunedIndexSet <- NULL
      ###
      ### DUST
      ###
      for(i in 3:length(indexSet))
      {
        pruneToDo <- TRUE
        s <- indexSet[i]
        if(t - s > 2) # if one value in the segment, don't prune !!!!!
        {
          r1 <- indexSet[i-2]
          r2 <- indexSet[i-1]
          {
            ### with R1
            mu <- compute_xstar(S, S2,
                                shift(r1), shift(s), shift(t),
                                costQ[shift(r1)], costQ[shift(s)], costQ[shift(t)])
            val <- evalDual_meanVar2(mu, S, S2,
                                     shift(r1), shift(s), shift(t),
                                     costQ[shift(r1)], costQ[shift(s)], costQ[shift(t)])
            if(val > 0)
            {
              if(pruneToDo == TRUE)
              {
                if(mu !=  0){dualPruning[2] <- dualPruning[2] + 1}
                else{dualPruning[1] <- dualPruning[1] + 1}
                pruneToDo <- FALSE
              }

              PrunedIndexSet <- c(PrunedIndexSet, s)
            }

            ### with R2
            mu <- compute_xstar(S, S2,
                                shift(r2), shift(s), shift(t),
                                costQ[shift(r2)], costQ[shift(s)], costQ[shift(t)])
            val <- evalDual_meanVar2(mu, S, S2,
                                     shift(r2), shift(s), shift(t),
                                     costQ[shift(r2)], costQ[shift(s)], costQ[shift(t)])
            if(val > 0)
            {
              if(pruneToDo == TRUE)
              {
                if(mu !=  0){dualPruning[3] <- dualPruning[3] + 1}
                else{dualPruning[1] <- dualPruning[1] + 1}
                pruneToDo <- FALSE
              }
              PrunedIndexSet <- c(PrunedIndexSet, s)
            }

            ### with R1 and R2
            coeff <- dualVALUE_meanVar2D(S, S2,
                                       shift(r1), shift(r2), shift(s), shift(t),
                                       costQ[shift(r1)], costQ[shift(r2)], costQ[shift(s)], costQ[shift(t)])
            val <- -Inf
            if(coeff[2]/(2*coeff[4]) > 0)
            {
              x <- (1/coeff[2])*((coeff[2]/(2*coeff[4]))  - coeff[1])
              val <- 0.5 * (1 + log(coeff[1] + coeff[2]*x)) - (coeff[3] + x*coeff[4])

              if(any(is.na(coeff) | is.infinite(coeff))){val <- -Inf}
              if(x < 0){val <- - Inf}
              if(coeff[5]*x + coeff[6] < 0){val <- - Inf}
            }
            if(val > 0)
            {
              if(pruneToDo == TRUE)
              {
                dualPruning[4] <- dualPruning[4] + 1
                pruneToDo <- FALSE
              }
              PrunedIndexSet <- c(PrunedIndexSet, s)
            }
          }
        }
      }

      ###
      ### PELT for smallest index OK
      ###
      s <- indexSet[1]
      if(t - s > 1) # if one value in the segment, don't prune !!!!!
      {
        val <- min_cost_meanVar(S, S2, shift(s), shift(t), costQ[shift(s)] + penalty)
        if(val > costQ[shift(t)] + penalty)
        {PrunedIndexSet <- c(PrunedIndexSet, s)}
      }
      ###
      ### new index set
      ###
      indexSet <- setdiff(indexSet, PrunedIndexSet)
    }
    ###########################
    ########################### Pruning step END
    ###########################

    nb[t] <- length(indexSet)
    indexSet <- c(indexSet, t) #add new test point
  }

  ########## backtracking changepoint ##########
  changepoints <- backtracking_changepoint(cp, n)
  ########## backtracking changepoint ##########

  return(list(changepoints = changepoints,
              nb = nb,
              dualPruning = dualPruning,
              lastIndexSet = indexSet,
              costQ = costQ[-1]))
}


##########################################################################################
##########################################################################################
##########################################################################################


#' dust_R_2param_regression
#'
#' @description DUST algorithm for regression problem
#' @param data a data frame of data of dimension n x 2 (type \code{"regression"}) generated by \code{dataGenerator_Reg} with column named \code{x} and \code{y}.
#' @param penalty penalty value (non-negative)
#' @param pruningOpt the pruning option to use. 0 = nothing, 1 = PELT, 2 = dust, 3 = PELT + dust
dust_R_2param_regression <- function(data, penalty, pruningOpt)
{
  ###
  ### preprocessing
  ###
  ### A,B,C,D,E,f used with the shift operator
  ###
  n <- nrow(data)
  A <- c(0, cumsum(data$x^2))
  B <- c(0, cumsum(data$x))
  C <- 0:n
  D <- -c(0, cumsum(data$x * data$y))
  E <- -c(0, cumsum(data$y))
  f <- c(0, cumsum(data$y^2))

  #########
  ###
  ### INITIALIZATION
  ###
  ### costQ used with the shift operator
  ###
  cp <- rep(0, n) #cp vector cp[k] = index of the last change-point for data y(1) to y(k)
  costQ <- rep(0, n + 1) # costQ[k] optimal cost for data y(1) to y(k-1)
  nb <- rep(0, n) # number of element to consider at each iteration
  costQ[1] <- -penalty
  indexSet <- 0
  #########
  ###
  ### update rule Dynamic Programming
  ###
  for(t in 1:n) # at t, transform Q_{t-1} into Q_{t}
  {
    min_temp <- Inf
    index <- 0 #if all eval = Inf (start of a segment)
    for(i in indexSet)
    {
      ### weight (t-i) --- cost Si..t = S_i+1 to S_t --- costQ[i] = Q_(i) (Q_0 = -penalty) ###
      eval <- min_cost_regression(A,B,C,D,E,f, shift(i), shift(t), costQ[shift(i)] + penalty)
      if(eval < min_temp){min_temp <- eval; index <- i}
    }
    costQ[shift(t)] <- min_temp
    cp[t] <- index

    ###########################
    ########################### Pruning step START
    ###########################

    if(length(indexSet) > 1)
    {
      PrunedIndexSet <- NULL ##### we keep the smallest index #####
      for(i in 1:length(indexSet))
      {
        s1 <- indexSet[i]

        if((pruningOpt == 2) || (pruningOpt == 3))  ### dust
        {
          if(i >= 2)  ### DUST
          {
            if(t - s1 > 1) # if one value in the segment, don't prune !!!!!
            {
              if(i == 2){s2 <- indexSet[1]}else{s2 <- sample(indexSet[1:(i-1)], 1)} # the index of the constraint (s2 < s1) and s1,s2 < t
              if(s1 - s2 > 1) ### REMOVE LATER
              {
                mu <- runif(1) * mu_max_2param(B, A, shift(s1), shift(s2), shift(t))# the mu value to test
                val <- evalDual_regression(mu, A,B,C,D,E,f, shift(s1), shift(s2), shift(t), costQ[shift(s1)] + penalty, costQ[shift(s2)] + penalty)
                if(val > costQ[shift(t)] + penalty){PrunedIndexSet <- c(PrunedIndexSet, s1)}
              }

            }
          }
        }

        if((pruningOpt == 1) || (pruningOpt == 3))  ### PELT additional pruning
        {
          if(t - s1 > 1) # if one value in the segment, don't prune !!!!!
          {
            val <- min_cost_regression(A,B,C,D,E,f, shift(s1), shift(t), costQ[shift(s1)] + penalty)
            if(val > costQ[shift(t)] + penalty){PrunedIndexSet <- c(PrunedIndexSet, s1)}
          }
        }

      }
      indexSet <- setdiff(indexSet, PrunedIndexSet)
    }
    ###########################
    ########################### Pruning step END
    ###########################

    nb[t] <- length(indexSet)
    indexSet <- c(indexSet, t) #add new test point
  }

  ########## backtracking changepoint ##########
  changepoints <- backtracking_changepoint(cp, n)
  ########## backtracking changepoint ##########

  return(list(changepoints = changepoints, nb = nb, lastIndexSet = indexSet, costQ = costQ[-1]))
}



